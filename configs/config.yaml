# @package _global_

defaults:
  - _self_


logging_and_checkpoint:
  eval_interval: 27 # epoch_size/(batch_size*devices)
  save_interval: 27 # epoch_size/(batch_size*devices)
  log_interval: ${data.batch_size} # batch_size
  mlflow_tracking_uri: "http://192.168.3.78:5100"
  experiment_name: "clinkart_Llama70B"
  run_name: "lora_8_devices"
  checkpoint_dir: ???

experiment:
  devices: 3
  num_epochs: 30
  learning_rate: 3e-5
  weight_decay: 0.1
  precision: "bf16-true"
  quantize: null
  out_dir: ???

  # computed on the fly
  # max_iters: num_epochs * (epoch_size // micro_batch_size) // devices
  # warmup_steps: (
  #   num_epochs 
  #   * (epoch_size // micro_batch_size)
  #   // devices
  #   // gradient_accumulation_iters
  # )

data:
  batch_size: 16 # divide the batch_size you want by devices
  micro_batch_size: 2
  max_seq_length: 256
  epoch_size: 839 # number of elements in the training set
  random_microbatch: False
  data_dir: ???
 
  # computed on the fly
  # gradient_accumulation_iters: batch_size // micro_batch_size

lora:
  r: 128
  alpha: 256
  dropout: 0.05
  query: True
  key: True
  value: True
  projection: True
  mlp: True
  head: True