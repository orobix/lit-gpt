# @package _global_

defaults:
  - _self_
  - hydra: default.yaml


logging_and_checkpoint:
  eval_interval: null # if null -> epoch_size/(batch_size*devices)
  save_interval: null # if null -> epoch_size/(batch_size*devices)
  log_interval: 16 # batch_size // micro_batch_size
  mlflow_tracking_uri: "http://192.168.3.78:5100"
  experiment_name: ???
  run_name: ???
  checkpoint_dir: ???

experiment:
  devices: 1 # the number of devices
  num_epochs: 30
  learning_rate: 3e-5
  weight_decay: 0.2
  precision: bf16-true
  quantize: bnb.nf4-dq
  out_dir: ???
  warmup_steps: 100

  # computed on the fly
  # max_iters: num_epochs * (epoch_size // micro_batch_size) // devices

data:
  batch_size: 32 # divide the batch_size you want by devices
  micro_batch_size: 2
  max_seq_length: 256
  valset_split_percentage: null
  random_microbatch: False
  balanced_batch_class:
    - NONE
  data_dir: data/data_from_docs_split_OL
  json:
    train_file_path: null
    val_file_path: null
  # computed on the fly
  # epoch_size: len(train_data) # number of elements in the training set
  # gradient_accumulation_iters: batch_size // micro_batch_size

eval:
  max_new_tokens: 100
  qualitative_val_sample_idx: 1
  temperature: 0

lora:
  r: 128
  alpha: 256
  dropout: 0.05
  query: True
  key: True
  value: True
  projection: True
  mlp: True
  head: True