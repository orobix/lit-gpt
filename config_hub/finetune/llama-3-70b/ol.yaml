checkpoint_dir: checkpoints/meta-llama/Meta-Llama-3-70B
out_dir: /home/michele.milesi/local/ecream/logs/runs/clinkart_Llama-3-70B/qlora_all_r128_balanced05_dropout005_wd002
precision: bf16-true
quantize: bnb.nf4-dq
devices: 1 # the number of devices
lora:
  r: 128
  alpha: 256
  dropout: 0.05
  query: True
  key: True
  value: True
  projection: True
  mlp: True
  head: True
data:
  class_path: litgpt.data.OL
  init_args:
    mask_prompt: False
    prompt_style: ol
    ignore_index: -100
    seed: 1337
    num_workers: 4
    download_dir: /home/michele.milesi/repos/lit-gpt/data/e3c
    file_name: gt_training_split_on_docs.json
    file_name_validation: gt_val_split_on_docs.json
    balance_cfg: 
      classes:
        - RELATION
      percentage: 0.5
train:
  save_interval: 40
  log_interval: 16
  global_batch_size: 32
  micro_batch_size: 2
  lr_warmup_steps: 100
  epochs: 20
  max_steps: null
  max_seq_length: 256
  learning_rate: 3e-5
  weight_decay: 0.02
  beta1: 0.9
  beta2: 0.95
logger_args:
  experiment_name: clinkart_Llama-3-70B
  run_name: qlora_all_r128_balanced05_dropout005_wd002
  tracking_uri: http://192.168.3.78:5100
  run_id: null
  synchronous: False
eval:
  interval: 40
  max_new_tokens: 100
  qualitative_val_sample_idx: 1
  temperature: 0
seed: 1337

# logging_and_checkpoint:
#   eval_interval: null # if null -> epoch_size/(batch_size*devices)
#   save_interval: null # if null -> epoch_size/(batch_size*devices)
#   log_interval: 16 # batch_size // micro_batch_size
#   mlflow_tracking_uri: "http://192.168.3.78:5100"
#   experiment_name: ???
#   run_name: ???
#   synchronous: False

# experiment:
#   num_epochs: 30
#   learning_rate: 3e-5
#   weight_decay: 0.1
#   warmup_steps: 100

#   # computed on the fly
#   # max_iters: num_epochs * (epoch_size // micro_batch_size) // devices

# data:
#   batch_size: 32 # divide the batch_size you want by devices
#   micro_batch_size: 2
#   max_seq_length: 256
#   valset_split_percentage: null
#   random_microbatch: False    
#   data_dir: data/data_from_docs_split_OL/llama
#   json:
#     train_file_path: null
#     val_file_path: null
#   # computed on the fly
#   # epoch_size: len(train_data) # number of elements in the training set
#   # gradient_accumulation_iters: batch_size // micro_batch_size

# eval:
  
